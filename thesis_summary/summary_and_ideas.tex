\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
%\geometry{
% a4paper,
% total={170mm,257mm},
% left=25mm, right=25mm,
% top=18mm, bottom=20mm,
%}

%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\usepackage[para]{footmisc}

% for landscape and long table
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{capt-of}% or use the larger `caption` package

\title{Progress Report: Interactive Machine Learning\\ for Information Visualization}
\author{Viet~Minh~Vu\\ under the supervision of Prof. Beno\^it~Fr\'enay}
\date{\today}


\begin{document}

\maketitle

\section{Scope of the Research Proposal}
The goal of the research proposal titled ``Interactive Machine Learning for Information Visualization'' is to use machine learning algorithms to visualize and analyze high~dimensional data.
The visualization is a representation in a 2-dimensional space of an ensemble of data points (a scatter plot).
This representation is often called the embedding in low dimensional or latent space.
It must reveal hidden structures of the input data and preserves its characteristics, such as similar points should stay close together or dissimilar points should stay apart in the visualization.
The algorithm is considered to be able to learn from the data, i.e., it can automatically produce a mapping to project the high dimensional data into a 2D space, if it can uncover the hidden patterns in the data which are useful for the user.
But instead of letting the \emph{dimensionality reduction} (DR) algorithms operate automatically, this work tries to integrate the user's interactions into the workflow.
Our goals are to take advantages of the human feedback to correct the errors of the learning system and to allow the users to manipulate the visualization according to their needs.
The expected visualization is not only algorithmically optimized but also understandable and explainable to the human.
Several research directions are explored in the first two years and the primary results are presented in Section~\ref{sec:results}. Our perspective and research plan for the next two years are presented in Section~\ref{sec:work}.


\section{Achievements during the First Two Years}\label{sec:results}
We started by reviewing over 50 existing interactive dimensionality reduction (DR) methods, then tried to integrate a simple type of constraint to $t$-Distributed Stochastic Neighbor Embedding ($t$-SNE), a well known DR method.
$t$-SNE is a powerful and flexible DR method but requires a difficult fine-tuning process to find its best hyper-parameter. 
We propose an interactive method that takes the user-selected pairwise constraints between objects and use them to select the best $t$-SNE visualization.
We also try to integrate the user's feedback on the positions of the points in the 2D space directly into a probabilistic DR method.
This approach leads us to a novel class of interactive probabilistic DR methods for our future research.


A systematic review of variety of existing \emph{visual analysis} methods is first conducted.
We consider the users' interactions as the \emph{constraints} to the DR methods.
For example, the users can interact directly with the visualization to fix the positions of their interested points or to make links to express similar/dissimilar points.
Most of the reviewed methods translate these cognitive feedbacks into logical constraints for the optimization procedure.
We plan to narrow down the scope of the reviewed methods to conduct a survey paper for a specified group of probabilistic DR methods which is our main research topic in the next two years.

Our first proposed method is to integrate the \emph{must-be-here} constraints on the positions of points into the widely used $t$-SNE method.
Via an interactive interface, the user can move some anchor points and fix their positions in the 2D scatter plot to indicate which points should stay close together or far apart.
This method can split a dense group or merge some small groups.
Technically, the constraints on the positions of the fixed points are formulated as a regularization term to be jointly optimized with the original objective function of $t$-SNE.
This approach has only a local effect to the points around the selected points.
This drawback suggests us to find a better regularization term to represent the constraints between objects in such a way that, e.g., the \emph{semantic similarity} between objects can be preserved.

The second proposed method tackles the problem of finding the best hyper-parameter for $t$-SNE.
We let the users define their requirements about their expected visualization in advance by specifying the relation between the data points in high dimensional space (such as a similar link connecting two \emph{visually similar} objects or a dissimilar link connecting two \emph{visually different} objects).
These pairwise constraints are then translated to a numerical score to rank the pre-calculated embeddings in order to find the best one.
This work was submitted to the highly ranked IEEE TVCG journal.
After receiving the comments and suggestions of the reviewers, we are working on improving the scalability and stability aspect and evaluating our method with the real user interactions to resubmit this work.

The third proposed method ``user-steering interpretable visualization with probabilistic principle component analysis (\emph{PPCA})'' is accepted for presentation and publication at the international \emph{ESANN} conference, which leads us to a new open research direction of probabilistic DR methods (with more details in the next section).
In this method, the user can manipulate some selected points in 2D space to create a more understandable embedding, i.e., it is easier to explain the meaning of the axes in the newly created visualization.

For this research project, we have proposed a solution which is implemented in the ESANN paper and proved its potential.
We use a probabilistic approach to formulate the users' constraints in a dimensionality reduction problem.
The visualization is indeed the latent positions of each data point in the low dimensional space which are modeled as the latent variables.
A generative process is defined to describe how to obtain the high dimensional data assuming that we know their corresponding latent positions.
This step is easier than (its invert direction of) finding a mapping to project the high dimensional data into a lower dimensional space as in the original DR method.
Our problem of finding the latent variables given the observed data is called inference problem, which can be solved efficiently under a Bayesian framework.

Our main concern is thus about modeling the user's feedbacks.
For example, in our proposed interactive \emph{PPCA}, the user's feedbacks on the positions of the selected points in the visualization are directly modeled as the prior distributions for the corresponding selected latent variables.
The positions of the other points are automatically inferred from the model.
In that way, the user can control the positions of some anchor points in order to manipulate the whole visualization.


\section{On Going Works with $t$-SNE}\label{sec:tsne}
\subsection{Integrating pairwise constraints to $t$-SNE}
The advantage of $t$-SNE is to preserve local structures and also global structure of the data.
Conceptually, we can say a visualization is easy to understand if we can see clearly the groups in it.
However, as pointed out in \emph{how to use $t$-SNE effectively}~\citep{wattenberg2016how}
% , one of the major difficulties when applying $t$-SNE is how to connect the intuition about the data in high dimensional space to its representation in 2-dimensional space. More specifically, 
``cluster sizes in a $t$-SNE plot mean nothing'' and ``distances between clusters might not mean anything''.
That means the within-cluster distances and between-clusters distances are not well preserved in the $t$-SNE embeddings.

A question can be raised: do we need a $t$-SNE embedding that reflects well the size of groups and relative distances between groups?
If yes, can we use the pairwise constraints between similar and dissimilar data points in high dimensional space to guide $t$-SNE to overcome the above drawback? 
A common approach to integrate the constraints is to represent them as a regularization term to be jointly optimized with the original objective function of $t$-SNE.
One can present the pairwise constraints in a simple form of \emph{semantic similarity}~\citep{logeswaran2018efficient}:
\[
	\sum_{x, x^+, x^-} \Big[
		- \log \frac{\exp \big(f(x)^T f(x^+) \big)}
		{\exp \big(f(x)^T f(x^+) \big) + \exp \big(f(x)^T f(x^-) \big)}
	\Big],
\]
where $f(x)$ is a representation function of a sample $x$, (can simply be a learned embedding of $t$-SNE),
$x^+$ and $x^-$ are the corresponding \emph{positive} and \emph{negative} samples of $x$ (can be easily selected by the user).
We can also experiment with another form of semantic similarity which is introduced in a recent theoretical analysis of \emph{contrastive} unsupervised representation learning~\citep{arora2019theoretical}.
Another way is to use \emph{triplet} constraints $(x,x^+,x^-)$ to represent $x$ is closer to $x^+$ than to $x^-$.
The following regularization term, aka. \emph{triplet loss}~\citep{schroff2015facenet} should be minimized.
\[
	\sum_{x, x^+, x^-} \Big( ||f(x) - f(x^+) ||^2 - || f(x) - f(x^-) ||^2 + margin \Big)
\]

\subsection{How to Find the Perplexity of $t$-SNE Automatically?}
As presented above, we have tried to use the user defined constraints on the similarity/dissimilarity of data points as a criterion to find the best visualization.
But this approach has a big drawback in its scalability.
We need to heuristically search through all precalculated embeddings to find the best one that maximizes the constraint preserving score.
A simple but efficient solution is to use a classical Bayesian optimization approach. (For a gentle introduction, see~\citep{brochu2010tutorial}).
Bayesian optimization (\emph{BayesOpt}) is an old method of find the maximum of any black-box, expensive cost function.
We can define our target function as a combination of the proposed constraint score and/or any visualization quality metric.
Our goal is thus to find the best visualization that respects the constraints of the user and also has a good quality metric measure.
BayesOpt builds a probabilistic model of the target function and uses it to select the most promising hyperparameters to evaluate.
In our recent experiments, instead of doing linear search with hundreds of possible hyperparameters, we need to test only dozens of them to find the best one.
The remaining work consists of choosing or combining the strategies to predict the next hyperparameter to try in BayesOpt.
The core element in BayesOpt is \emph{Gaussian Process} model, which will be detailed in the next section.


\section{Research Topics on Probabilistic DR}\label{sec:work}
As described above, our current approach is to incorporate the user constraints into some basic DR methods.
An inconvenience with this approach is that, we have to find different forms for the regularization term to represent different types of constraints for different basic DR methods.
Inspired by the Bayesian visual analytics (\emph{BaVA}) framework~\underline{\citep{house2015bayesian}} and the model-based approach~\citep{bishop2013model}, we would like to create an unified framework for modeling the users' feedback for the probabilistic DR methods.
The model-based approach separates the model (how we represent our problem) and the algorithm (how we fit the model with the observed data to infer the latent structures that we desire to find).
Taking advantages of the recent powerful probabilistic programming toolbox (like Stan\footnote{https://mc-stan.org/}, Pyro\footnote{http://pyro.ai/} or TensorFlow Probability\footnote{https://www.tensorflow.org/probability}), we thus do not have to worry about the inference problem.
In fact it still takes time to make the inference procedure work in practice, but our main research problem is now focused on how to model the users' feedback in a probabilistic framework.
\underline{\citep{lawrence2005probabilistic}} reviews several probabilistic DR methods including density network, generative topographic mapping, PPCA and relates them to popular spectral techniques such as kernel PCA and multidimensional scaling. (This is a key paper to start with).
We then plan to work with two latent variables models families: \emph{gaussian process latent variable models} (Section~\ref{sec:gp}) and \emph{auto-encoding variational bayes} (Section~\ref{sec:nn}).
The idea and motivation for this approach will be detailed in the following sections.


\subsection{Using \emph{Gaussian Processes} for Modeling the User's Feedbacks}\label{sec:gp}
We have proposed a simple idea of integrating the users' feedback on the position of points in the visualization into PPCA model.
This model can be improved by making it more complex (e.g., mixture of PPCA\citep{tipping1999mixtures}) or making it richer (e.g., using \emph{Gaussian process}~\citep{rasmussen2005gp} to model the latent variables).
For the first idea with the mixture model, the users can give their feedback about the group of the interacted points, e.g., which points should be in the same or in different groups.
This type of feedback can be used to create the visualization with semantic clustering in which the clusters respect the requirements of the user instead of the annotated class labels.

The second idea of using Gaussian process is more potential.
A Gaussian process, roughly speaking, is a collection of random variables, any finite number of which have joint Gaussian distributions.
It is usually used as a distribution over functions.
In PPCA model, the data is described by generative process, which is indeed a mapping function from a latent space to the observed data space.
Gaussian Process Latent Variable Models (GP-LVM)~\underline{\citep{lawrence2004gaussian}} defines a particular Gaussian process prior on this mapping function in order to learn a more general non-linear mapping function for the data.

Moreover, when the user interacts with points in the visualization, the covariance function can be used to explain how the selected points and all other non-touched points are correlated to each other.
In the Gaussian process model, the covariance function encodes all assumptions about the form of the mapping function that describes the data.
We would like to use this covariance function to also encodes the user constraints.
With this idea, we can model observation-level interaction~\citep{endert2011observation} into GP-LVM model.
More specifically, our research question is how to present the constraints of individual observations (fixed position of anchor points, pairwise constraints or constraints on group of points, etc.) by a covariance function of the Gaussian process models? 

\subsection{Interactive Visualization in (Deep) Neural Network Model}\label{sec:nn}
The classical probabilistic models like PPCA or GP-LVM have an issue to not scale well for the large dataset.
Even though these models are flexible (e.g., we can choose a periodic kernel for GP-LVM to work with periodic time-series data), we still need a more powerful model to work directly with the more complex data like image or text.
Variational Auto-encoder (\emph{VAE})~\underline{\citep{kingma2013auto}} leverages the deep neural network for learning the representation features and has several advantages.
The auto-encoder network can be modeled as a probabilistic model and the inference problem is solved efficiently by a stochastic variational approach (which converts an inference problem to an optimization problem and solves it approximately by a gradient-based method).
That gives us enough flexibility to choose the network architecture and makes this model work with large dataset.

VAE does not only learn the latent representation of the data but also generate the new realistic samples.
Taking the ideas of semi-supervised VAE~\citep{kingma2014semi} or Conditional VAE~\citep{sohn2015learning}, we can see a possible usage of the user's interaction to control the structure of the latent representation or the generated samples.
If the user can modify the value of the latent features and see the change in the corresponding generated sample, he can gain knowledge about the meaning of the latent features.
Towards the interpretability of the generative model, one can visualize the learned features in each layer of the network to understand the contribution of the latent features to the output generated sample.
The guideline for this research direction is a collection of interactive articles on \emph{Feature Visualization}~\underline{\citep{olah2017feature}} and \emph{The Building Blocks of Interpretability}~\underline{\citep{olah2018the}} in the \emph{Distill} journal.


\medskip

\small{
% \bibliographystyle{unsrt}
\bibliography{refs.bib}
}

% \afterpage{%
%     \clearpage%
%     \begin{landscape}
%         \section*{Timeline in the first two years}
%         \centering
%         \begin{longtable}{p{.07\linewidth}|p{.16\linewidth}|p{.40\linewidth}|p{.30\linewidth}}

%             Time & Work & Summary & Result \\
%             \hline \hline

%             09/2017 - 12/2017 (4m) & Survey on constraints in DR methods. & Reading SOTA, focusing on the user's feedbacks which can be represented as constraints for the DR methods. & A survey paper, but having two serious problems: the scope is too large and the survey is not well structured. (Pending) \\ 
%             %&&&\\

%             01/2018 - 03/2018 (3m) & Integrating must-be-here constraints to tSNE. & The feedback on the positions of some control points is formulated as regularization term to jointly optimized with the objective function of tSNE. & A prototype (1) allowing the use to interact directly with the visualization (to move the control points) and (2) evaluating the user's feedbacks in form of different regularization types. (Stopped) \\
%             %&&&\\

%             04/2018 - 07/2018 (4m) & Using user constraints to select the best visualization of tSNE. & Introduce the \emph{user-preserving scores} to quantify the user's feedbacks in form of similar/dissimilar pairs of points and use this score to heuristically find the best tSNE embedding. & The proposed score with auto-generated constraints agrees with several visualization quality metrics and shows the potential results with some datasets. A \emph{TVCG} paper is rejected. \\
%             %&&&\\

%             08/2018 - 11/2018 (4m) & User-steering visualization with \emph{PPCA}. & Construct an interactive Probabilistic PCA model which allows the users to set positions for some selected points in the embedding. These feedbacks are translated into a Bayesian framework in form of prior knowledge on some control points in the latent (embedding) space. The resulting positions of all other points will be inferred automatically. & Integrating the user's feedbacks directly into a probabilistic model (in a Bayesian framework) is a potential research direction. An ESANN paper is accepted. \\
%             %&&&\\

%             12/2018 - 02/2019 (3m) & Mixture of Probabilistic PCA model. & Make the PPCA model more powerful by introducing a mixture of many of independent PPCA. A long-term goal is to do clustering in latent space and do dimensionality reduction at the same time. The expected result is the position of data points in latent space and also theirs predicted labels in order to distinguish the group in the visualization. & Trying different inference methods, including solving the MAP problem analytically, using black-box variational inference and MCMC toolbox, but still do not obtain the good results. (WIP) \\
%             %&&&\\

%             03/2019 - 04/2019 (2m) & Rework on the rejected TVCG paper. & Try to solve two big remaining problems on the scalability/stability of tSNE and the evaluation with real user interactions. & Propose chain-tSNE and a prototype allowing the user to select and observe the similar/dissimilar pairs visually. However still seeing some problems with the proposed scores with the manual constraints. (WIP) \\
%         %\caption{The timeline of works in the first 20 months.}
%         \end{longtable}


%     \end{landscape}
%     \clearpage%
% } % afterpage

\end{document}

