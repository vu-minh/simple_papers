\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
%\geometry{
% a4paper,
% total={170mm,257mm},
% left=25mm, right=25mm,
% top=18mm, bottom=20mm,
%}

%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

% for landscape and long table
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{capt-of}% or use the larger `caption` package

\title{Progress Report: Interactive Machine Learning\\ for Information Visualization}
\author{Viet~Minh~Vu\\ under the supervision of Prof. Beno\^it~Fr\'enay}
\date{\today}


\begin{document}
 
\maketitle

\section{Scope of the Research Proposal}
The goal of the research proposal titled ``Interactive Machine Learning for Information Visualization'' is to use machine learning algorithms to visualize and analyze high~dimensional data.
The visualization is a representation in a 2-dimensional space of an ensemble of data points (a scatter plot).
This representation is often called the embedding in low dimensional or latent space.
It must reveal hidden structures of the input data and preserves its characteristics, such as similar points should stay close together or dissimilar points should stay apart in the visualization.
The algorithm is considered to be able to learn from the data, i.e., it can automatically produce a mapping to project the high dimensional data into a 2D space, if it can uncover the hidden patterns in the data which are useful for the user.
But instead of letting the \emph{dimensionality reduction} (DR) algorithms operate automatically, this work tries to integrate the user's interactions into the workflow.
Our goals are to take advantages of the human feedback to correct the errors of the learning system and to allow the users to manipulate the visualization according to their needs.
The expected visualization is not only algorithmically optimized but also understandable and explainable to the human.
Several research directions are explored in the first two years and the primary results are presented in Section~\ref{sec:results}. Our perspective and research plan for the next two years are presented in Section~\ref{sec:work}.


\section{Achievements during the First Two Years}\label{sec:results}
We started by reviewing over 50 existing interactive dimensionality reduction (DR) methods, then tried to integrate a simple type of constraint to $t$-Distributed Stochastic Neighbor Embedding ($t$-SNE), a well known DR method.
$t$-SNE is a powerful and flexible DR method but requires a difficult fine-tuning process to find its best hyper-parameter. 
We propose an interactive method that takes the user-selected pairwise constraints between objects and use them to select the best $t$-SNE visualization.
We also try to integrate the user's feedback on the positions of the points in the 2D space directly into a probabilistic DR method.
This approach leads us to a novel class of interactive probabilistic DR methods for our future research.
 

A systematic review of variety of existing \emph{visual analysis} methods is first conducted.
We consider the users' interactions as the \emph{constraints} to the DR methods.
For example, the users can interact directly with the visualization to fix the positions of their interested points or to make links to express similar/dissimilar points.
Most of the reviewed methods translate these cognitive feedbacks into logical constraints for the optimization procedure.
We plan to narrow down the scope of the reviewed methods to conduct a survey paper for a specified group of probabilistic DR methods which is our main research topic in the next two years.

Our first proposed method is to integrate the \emph{must-be-here} constraints on the positions of points into the widely used $t$-SNE method.
Via an interactive interface, the user can move some anchor points and fix their positions in the 2D scatter plot to indicate which points should stay close together or far apart.
This method can split a dense group or merge some small groups.
Technically, the constraints on the positions of the fixed points are formulated as a regularization term to be jointly optimized with the original objective function of $t$-SNE.
This approach has only a local effect to the points around the selected points.
This drawback suggests us to find a better regularization term to represent the constraints between objects in such a way that, e.g., the \emph{semantic similarity} between objects can be preserved.

The second proposed method tackles the problem of finding the best hyper-parameter for $t$-SNE.
We let the users define their requirements about their expected visualization in advance by specifying the relation between the data points in high dimensional space (such as a similar link connecting two \emph{visually similar} objects or a dissimilar link connecting two \emph{visually different} objects).
These pairwise constraints are then translated to a numerical score to rank the pre-calculated embeddings in order to find the best one.
This work was submitted to the highly ranked IEEE TVCG journal.
After receiving the comments and suggestions of the reviewers, we are working on improving the scalability and stability aspect and evaluating our method with the real user interactions to resubmit this work.

The third proposed method ``user-steering interpretable visualization with probabilistic principle component analysis (\emph{PPCA})'' is accepted for presentation and publication at the international \emph{ESANN} conference, which leads us to a new open research direction of probabilistic DR methods (with more details in the next section).
In this method, the user can manipulate some selected points in 2D space to create a more understandable embedding, i.e., it is easier to explain the meaning of the axes in the newly created visualization.


\section{On Going Works with $t$-SNE}\label{sec:tsne}
The advantage of $t$-SNE is to preserve local structures and also global structure of the data.
Conceptually, we can say a visualization is easy to understand if we can see clearly the groups in it.
However, as pointed out in \emph{how to use $t$-SNE effectively}~\citep{wattenberg2016how}
% , one of the major difficulties when applying $t$-SNE is how to connect the intuition about the data in high dimensional space to its representation in 2-dimensional space. More specifically, 
``cluster sizes in a t-SNE plot mean nothing'' and ``distances between clusters might not mean anything''.
That means the within-cluster distances and between-clusters distances are not well preserved in the $t$-SNE embeddings.

A question can be raised: do we need a $t$-SNE embedding that reflects well the size of groups and relative distances between groups?
If yes, can we use the pairwise constraints between similar and dissimilar data points in high dimensional space to guide $t$-SNE to overcome the above drawback? 
A common approach to integrate the constraints is to present them as a regularization term to jointly optimized with the original objective function of $t$-SNE.
As summarized in a recent theoretical analysis of \emph{contrastive} unsupervised representation learning~\citep{arora2019theoretical}, 
one can present the pairwise constraints in form of \emph{semantic similarity}
\[
    - \log \frac{\exp \big(f(x)^T f(x^+) \big)}
                {\exp \big(f(x)^T f(x^+) \big) + \exp \big(f(x)^T f(x^-) \big)},
\]
where $f(x)$ is a representation function of a sample $x$, $x^+$ and $x^-$ are its corresponding \emph{positive} and \emph{negative} samples, which can be easily selected by the user.
Another way is to use \emph{triplet} constraints $(x,x^+,x^-)$ to represent $x$ is closer to $x^+$ than to $x^-$. The regularization term $||f(x) - f(x^+) ||^2 - || f(x) - f(x^-) ||^2$, aka triplet loss~\citep{schroff2015facenet} should be minimized.

Another problem is how to find the perplexity for $t$-SNE automatically?
As presented above, we have tried to use the user defined constraints on the similarity/dissimilarity of data points as a criterion to find the best visualization.
But this approach has a big drawback in its scalability.
We need to heuristically search through all pre-calculated embeddings to find the best one that maximizes the constraint preserving score.
A simple but efficient solution is to use a classical Bayesian optimization approach. (For a gentel introduction, see~\citep{brochu2010tutorial}).
Bayesian optimization (\emph{BayesOpt}) is an old method of find the maximum of any black-box, expensive cost function.
We can define our target function as a combination of the proposed constraint score and/or any visualization quality metric.
Our goal is thus to find the best visualization that respects the constraints of the user and also has a good quality metric measure.
BayesOpt builds a probabilistic model of the target function and uses it to select the most promissing hyperparameters to evaluate.
In our recent experiments, instead of doing linear search with hundreds of possible hyperparameters, we need to test only dozens of them to find the best one.
The remaining work consists of choosing or combining the stategies to predict the next hypterparameter to try in BayesOpt.
The core element in BayesOpt that we use is \emph{Gaussian Process} model, which will be detailed in the next section.


\section{Research Plan for the Next Two Years}\label{sec:work}
Our current research topic is on incorporating the user constraints in to some basic DR methods.
In order to finish the two on going works on $t$-SNE, we propose some ideas in Section \ref{sec:tsne}.
+ Bayesian visual analytics (\emph{BaVA}) framework~\citep{house2015bayesian}

As described above, our current approach is to incorporate the user constraints into some basic DR methods.
This discrete approach has been well studied (with many visual analysis methods reviewed in our previous work), but there still exists an inconvenience.
The representation of the user constraints is tied to the objective function of the selected DR method.
That is why we found a wide variety of specifically designed methods for different types of constraints.
This difficulty forces us to deal with two separate problems at the same time: users' feedbacks representation and constrained optimization problem.

For this research project, we have proposed a solution which is implemented in the ESANN paper and proved its potential.
We use a probabilistic approach to formulate the users' constraints in a dimensionality reduction problem.
The visualization is indeed the latent positions of each data point in the low dimensional space which are modeled as the latent variables.
A generative process is defined to describe how to obtain the high dimensional data assuming that we know their corresponding latent positions.
This step is easier than (its invert direction of) finding a mapping to project the high dimensional data into a lower dimensional space as in the original DR method.
Our problem of finding the latent variables given the observed data is called inference problem, which can be solved efficiently under a Bayesian framework.
% Let start from our initial belief about the latent variables.
% The more data we observe, the closer to the real values of the latent variables we can obtain.
% Leveraging the power of Bayesian framework, we strengthen our initial belief about the latent variables in the light of given data and obtain a model which can tell us the inferred latent positions of each data point and also how certain the model is.

Our main concern is thus about modeling the user's feedbacks.
For example, in our proposed interactive \emph{PPCA}, the user's feedbacks on the positions of the selected points in the visualization are directly modeled as the prior distributions for the corresponding selected latent variables.
The positions of the other points are automatically inferred from the model.
In that way, the user can control the positions of some anchor points in order to manipulate the whole visualization.

We plan to work on the two following research questions.
(1) How to deeply integrate the user's constraints into a probabilistic model?
(2) How to evaluate the proposed model in an interactive context with online feedbacks of the real users?
An interesting aspect in this approach is that, we can benefit from a high-capacity representation function (i.e., a neural network as in the \emph{autoencoder} method) for the generative process and also from a high efficient optimization toolbox to solve the inference problem (i.e., an universal \emph{variational inference} method).
% That helps us stay focused on the main research questions without worrying about the underlying complex optimization process.
Modeling the user's constraints in a \emph{variational autoencoder} framework is a typical problem we are now starting to work on.

\subsection{Using \emph{Gaussian Processes} for Modeling the User's Feedbacks}\label{sec:prob_dr}
Key papers:
+ Gaussian Process Latent Variable Models (GP-LVM)
+ Probabilistic Non-linear PCA with GP-LVM

\subsection{Interactive Visualization in Deep Learning Model}\label{sec:nn}
+ distilpub
+ viz pre-learned model (ICLR)
+ VAE
+ Semisupervised VAE


\medskip
 
% \bibliographystyle{unsrt}
\bibliography{refs.bib}


% \afterpage{%
%     \clearpage%
%     \begin{landscape}
%         \section*{Timeline in the first two years}
%         \centering
%         \begin{longtable}{p{.07\linewidth}|p{.16\linewidth}|p{.40\linewidth}|p{.30\linewidth}}

%             Time & Work & Summary & Result \\
%             \hline \hline

%             09/2017 - 12/2017 (4m) & Survey on constraints in DR methods. & Reading SOTA, focusing on the user's feedbacks which can be represented as constraints for the DR methods. & A survey paper, but having two serious problems: the scope is too large and the survey is not well structured. (Pending) \\ 
%             %&&&\\

%             01/2018 - 03/2018 (3m) & Integrating must-be-here constraints to tSNE. & The feedback on the positions of some control points is formulated as regularization term to jointly optimized with the objective function of tSNE. & A prototype (1) allowing the use to interact directly with the visualization (to move the control points) and (2) evaluating the user's feedbacks in form of different regularization types. (Stopped) \\
%             %&&&\\

%             04/2018 - 07/2018 (4m) & Using user constraints to select the best visualization of tSNE. & Introduce the \emph{user-preserving scores} to quantify the user's feedbacks in form of similar/dissimilar pairs of points and use this score to heuristically find the best tSNE embedding. & The proposed score with auto-generated constraints agrees with several visualization quality metrics and shows the potential results with some datasets. A \emph{TVCG} paper is rejected. \\
%             %&&&\\
            
%             08/2018 - 11/2018 (4m) & User-steering visualization with \emph{PPCA}. & Construct an interactive Probabilistic PCA model which allows the users to set positions for some selected points in the embedding. These feedbacks are translated into a Bayesian framework in form of prior knowledge on some control points in the latent (embedding) space. The resulting positions of all other points will be inferred automatically. & Integrating the user's feedbacks directly into a probabilistic model (in a Bayesian framework) is a potential research direction. An ESANN paper is accepted. \\
%             %&&&\\

%             12/2018 - 02/2019 (3m) & Mixture of Probabilistic PCA model. & Make the PPCA model more powerful by introducing a mixture of many of independent PPCA. A long-term goal is to do clustering in latent space and do dimensionality reduction at the same time. The expected result is the position of data points in latent space and also theirs predicted labels in order to distinguish the group in the visualization. & Trying different inference methods, including solving the MAP problem analytically, using black-box variational inference and MCMC toolbox, but still do not obtain the good results. (WIP) \\
%             %&&&\\
            
%             03/2019 - 04/2019 (2m) & Rework on the rejected TVCG paper. & Try to solve two big remaining problems on the scalability/stability of tSNE and the evaluation with real user interactions. & Propose chain-tSNE and a prototype allowing the user to select and observe the similar/dissimilar pairs visually. However still seeing some problems with the proposed scores with the manual constraints. (WIP) \\
%         %\caption{The timeline of works in the first 20 months.}
%         \end{longtable}


%     \end{landscape}
%     \clearpage%
% } % afterpage

\end{document}

